{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPdFmbAmggLswduynyziwl0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anzar00/UNet-Models/blob/main/Major_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UNET"
      ],
      "metadata": {
        "id": "yQi8AXlc6CKp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "VG_MIwR83UuI"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_unet(input_shape=(584, 565, 3), num_classes=1):\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    \n",
        "    # Encoder block\n",
        "    c1 = keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
        "    c1 = keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(c1)\n",
        "    p1 = keras.layers.MaxPooling2D((2, 2))(c1)\n",
        "    \n",
        "    c2 = keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(p1)\n",
        "    c2 = keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c2)\n",
        "    p2 = keras.layers.MaxPooling2D((2, 2))(c2)\n",
        "    \n",
        "    c3 = keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(p2)\n",
        "    c3 = keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(c3)\n",
        "    p3 = keras.layers.MaxPooling2D((2, 2))(c3)\n",
        "    \n",
        "    # Decoder block\n",
        "    u4 = keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='valid')(p3)\n",
        "    u4 = keras.layers.concatenate([u4, c2])\n",
        "    c4 = keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(u4)\n",
        "    c4 = keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c4)\n",
        "    \n",
        "    u5 = keras.layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='valid')(c4)\n",
        "    u5 = keras.layers.concatenate([u5, c1])\n",
        "    c5 = keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(u5)\n",
        "    c5 = keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(c5)\n",
        "    \n",
        "    outputs = keras.layers.Conv2D(num_classes, (1, 1), activation='sigmoid')(c5)\n",
        "    \n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    \n",
        "    return model"
      ],
      "metadata": {
        "id": "C14FjrQo5HHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model\n",
        "model = build_unet()"
      ],
      "metadata": {
        "id": "qk_HV9La5VBl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "outputId": "a403ec6b-7b7e-4e1d-f2fb-1f699cc551d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-a0b86779bf44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Build the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_unet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-a4c66ef0a6a9>\u001b[0m in \u001b[0;36mbuild_unet\u001b[0;34m(input_shape, num_classes)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Decoder block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mu4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2DTranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mu4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mc4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'same'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mc4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'same'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/layers/merging/concatenate.py\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(inputs, axis, **kwargs)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0mA\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mconcatenation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minputs\u001b[0m \u001b[0malongside\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \"\"\"\n\u001b[0;32m--> 231\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mConcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/layers/merging/concatenate.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    129\u001b[0m                 )\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique_dims\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_merge_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: A `Concatenate` layer requires inputs with matching shapes except for the concatenation axis. Received: input_shape=[(None, 146, 140, 64), (None, 292, 282, 64)]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "S6mrWuJE5Wlm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "outputId": "e2635b52-cf9f-4b15-c0fa-d3440d670e6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-93db3f1ab3fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Compile the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the DRIVE dataset\n",
        "(X_train, y_train), (X_test, y_test) = keras.datasets.drive(num_classes=1, load_data=True)"
      ],
      "metadata": {
        "id": "U323-0k15dyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the model\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "id": "VbArC8-D5jf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test Loss:', test_loss)\n",
        "print('Test Accuracy:', test_acc)"
      ],
      "metadata": {
        "id": "URv-Kiw25nPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting the Training and Validation Accuracy \n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(10)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "64qFQjOu5t3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Using the Model for Segmentation\n",
        "\n",
        "# Load a new image from the DRIVE dataset\n",
        "new_image = X_test[0]\n",
        "new_image = np.expand_dims(new_image, axis=0)\n",
        "\n",
        "# Predict the segmentation mask\n",
        "segmentation_mask = model.predict(new_image)\n",
        "segmentation_mask = np.squeeze(segmentation_mask)\n",
        "\n",
        "# Plot the original image and the segmentation mask\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(X_test[0], cmap='gray')\n",
        "plt.title('Original Image')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(segmentation_mask, cmap='gray')\n",
        "plt.title('Segmentation Mask')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qQa06YYi5zhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the DRIVE dataset\n",
        "X = np.load('/content/drive/My Drive/DRIVE/images.npy')\n",
        "y = np.load('/content/drive/My Drive/DRIVE/masks.npy')\n",
        "\n",
        "# Preprocess the DRIVE data\n",
        "X = X / 255.0\n",
        "y = (y > 0.5).astype(int)\n",
        "\n",
        "# Build the UNet model\n",
        "model = build_unet(input_shape=(512, 512, 1), num_classes=1)\n",
        "\n",
        "# Compile the UNet model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the UNet model\n",
        "history = model.fit(X, y, epochs=50, validation_split=0.1)\n",
        "\n",
        "# Evaluate the UNet model\n",
        "plot_history(history)\n",
        "\n",
        "# Use the UNet model for segmentation\n",
        "pred = model.predict(X_test)\n"
      ],
      "metadata": {
        "id": "8pERno3s68jC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test 1"
      ],
      "metadata": {
        "id": "PX1WIpZaB6HK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from skimage.io import imread\n",
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D, Concatenate, BatchNormalization, Activation\n",
        "\n",
        "# Load the DRIVE dataset\n",
        "def load_dataset(data_path, folder):\n",
        "    images = []\n",
        "    masks = []\n",
        "    \n",
        "    for image_path, mask_path in zip(sorted(os.listdir(os.path.join(data_path, folder, 'images'))), \n",
        "                                      sorted(os.listdir(os.path.join(data_path, folder, 'masks')))):\n",
        "        images.append(imread(os.path.join(data_path, folder, 'images', image_path)))\n",
        "        masks.append(imread(os.path.join(data_path, folder, 'masks', mask_path)))\n",
        "        \n",
        "    return np.array(images), np.array(masks)\n",
        "\n",
        "# Load the training dataset\n",
        "X_train, y_train = load_dataset('/content/drive/My Drive/dataset', 'training')\n",
        "\n",
        "# Load the test dataset\n",
        "X_test, y_test = load_dataset('/content/drive/My Drive/dataset', 'test')\n",
        "\n",
        "# Define the UNet model\n",
        "def unet(input_shape):\n",
        "    inputs = Input(input_shape)\n",
        "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
        "    conv1 = BatchNormalization()(conv1)\n",
        "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n",
        "    conv1 = BatchNormalization()(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "    \n",
        "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n",
        "    conv2 = BatchNormalization()(conv2)\n",
        "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n",
        "    conv2 = BatchNormalization()(conv2)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "    \n",
        "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\n",
        "    conv3 = BatchNormalization()(conv3)\n",
        "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n",
        "    conv3 = BatchNormalization()(conv3)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "    \n",
        "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)\n",
        "    conv4 = BatchNormalization()(conv4\n"
      ],
      "metadata": {
        "id": "znETfr35B4Lb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rQoUklrIdoCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check New"
      ],
      "metadata": {
        "id": "kZ7W2-Mbdqsl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5k25zVf_eEpF",
        "outputId": "6de1196e-d849-405e-8b86-24bba45407f6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/dataset/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbBMq5kveSKV",
        "outputId": "4681e0aa-2b3b-473d-94c5-ce45c69a71e6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.6.0\n",
        "!pip install numpy==1.20\n",
        "!pip install matplotlib\n",
        "!pip install opencv-python-headless\n",
        "!pip install imagecodecs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l24bZfFH6-iU",
        "outputId": "f604abb3-6570-4c52-c719-50dd178221a6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==2.6.0\n",
            "  Downloading tensorflow-2.6.0-cp38-cp38-manylinux2010_x86_64.whl (458.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m458.4/458.4 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.6.0) (1.6.3)\n",
            "Collecting clang~=5.0\n",
            "  Downloading clang-5.0.tar.gz (30 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.6.0) (3.3.0)\n",
            "Collecting termcolor~=1.1.0\n",
            "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.6.0) (0.38.4)\n",
            "Collecting typing-extensions~=3.7.4\n",
            "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.6.0) (3.1.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.6.0) (0.2.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.6.0) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.6.0) (3.19.6)\n",
            "Collecting wrapt~=1.12.1\n",
            "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tensorflow-estimator~=2.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.6.0) (2.11.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.6.0) (2.11.2)\n",
            "Collecting keras-preprocessing~=1.1.2\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 KB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: grpcio<2.0,>=1.37.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.6.0) (1.51.3)\n",
            "Collecting absl-py~=0.10\n",
            "  Downloading absl_py-0.15.0-py3-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 KB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy~=1.19.2\n",
            "  Downloading numpy-1.19.5-cp38-cp38-manylinux2010_x86_64.whl (14.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.9/14.9 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.6.0) (0.4.0)\n",
            "Requirement already satisfied: keras~=2.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.6.0) (2.11.0)\n",
            "Collecting flatbuffers~=1.12.0\n",
            "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow==2.6.0) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow==2.6.0) (0.6.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow==2.6.0) (2.25.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow==2.6.0) (2.2.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow==2.6.0) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow==2.6.0) (2.16.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow==2.6.0) (3.4.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow==2.6.0) (57.4.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.6.0) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.6.0) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.6.0) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.6.0) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.6.0) (6.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.6.0) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.6.0) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.6.0) (1.26.14)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.6.0) (2.10)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.8/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.6->tensorflow==2.6.0) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.6.0) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.6.0) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.6.0) (3.2.2)\n",
            "Building wheels for collected packages: clang, termcolor, wrapt\n",
            "  Building wheel for clang (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clang: filename=clang-5.0-py3-none-any.whl size=30694 sha256=c2f2ac2173a9496bcedb57c91d7c9460c43bde331979c11384ab2f4c975af106\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/60/77/22b9b5887bd47801796a856f47650d9789c74dc3161a26d608\n",
            "  Building wheel for termcolor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4849 sha256=5e7a9d6e809dd494b4a9f5ed9d82e912f9b97b48e9059f3a23540b1e4afd7674\n",
            "  Stored in directory: /root/.cache/pip/wheels/a0/16/9c/5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wrapt: filename=wrapt-1.12.1-cp38-cp38-linux_x86_64.whl size=78580 sha256=8772b25d60f4de6c1c19ab7eb1aeb1a739a6bbeda34a78a67a47e6d36e96b13f\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/fd/9e/b6cf5890494cb8ef0b5eaff72e5d55a70fb56316007d6dfe73\n",
            "Successfully built clang termcolor wrapt\n",
            "Installing collected packages: wrapt, typing-extensions, termcolor, flatbuffers, clang, numpy, absl-py, keras-preprocessing, tensorflow\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.15.0\n",
            "    Uninstalling wrapt-1.15.0:\n",
            "      Successfully uninstalled wrapt-1.15.0\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "  Attempting uninstall: termcolor\n",
            "    Found existing installation: termcolor 2.2.0\n",
            "    Uninstalling termcolor-2.2.0:\n",
            "      Successfully uninstalled termcolor-2.2.0\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 23.1.21\n",
            "    Uninstalling flatbuffers-23.1.21:\n",
            "      Successfully uninstalled flatbuffers-23.1.21\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.22.4\n",
            "    Uninstalling numpy-1.22.4:\n",
            "      Successfully uninstalled numpy-1.22.4\n",
            "  Attempting uninstall: absl-py\n",
            "    Found existing installation: absl-py 1.4.0\n",
            "    Uninstalling absl-py-1.4.0:\n",
            "      Successfully uninstalled absl-py-1.4.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.11.0\n",
            "    Uninstalling tensorflow-2.11.0:\n",
            "      Successfully uninstalled tensorflow-2.11.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xarray 2022.12.0 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\n",
            "xarray-einstats 0.5.1 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\n",
            "pydantic 1.10.5 requires typing-extensions>=4.2.0, but you have typing-extensions 3.7.4.3 which is incompatible.\n",
            "jaxlib 0.4.4+cuda11.cudnn82 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\n",
            "jax 0.4.4 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\n",
            "cmdstanpy 1.1.0 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\n",
            "bokeh 2.4.3 requires typing-extensions>=3.10.0, but you have typing-extensions 3.7.4.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed absl-py-0.15.0 clang-5.0 flatbuffers-1.12 keras-preprocessing-1.1.2 numpy-1.19.5 tensorflow-2.6.0 termcolor-1.1.0 typing-extensions-3.7.4.3 wrapt-1.12.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting numpy==1.20\n",
            "  Downloading numpy-1.20.0-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.4/15.4 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.6.0 requires numpy~=1.19.2, but you have numpy 1.20.0 which is incompatible.\n",
            "cmdstanpy 1.1.0 requires numpy>=1.21, but you have numpy 1.20.0 which is incompatible.\n",
            "bokeh 2.4.3 requires typing-extensions>=3.10.0, but you have typing-extensions 3.7.4.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.20.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (3.5.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (8.4.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (4.38.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7->matplotlib) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.8/dist-packages (4.7.0.72)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.8/dist-packages (from opencv-python-headless) (1.20.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting imagecodecs\n",
            "  Downloading imagecodecs-2023.1.23-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.2/36.2 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from imagecodecs) (1.20.0)\n",
            "Installing collected packages: imagecodecs\n",
            "Successfully installed imagecodecs-2023.1.23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.6.0 numpy matplotlib opencv-python-headless imagecodecs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1qGvLOFxlL4",
        "outputId": "5bab1453-9a99-46ea-e607-86539d641d2c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow==2.6.0 in /usr/local/lib/python3.8/dist-packages (2.6.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (1.20.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (3.5.3)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.8/dist-packages (4.7.0.72)\n",
            "Requirement already satisfied: imagecodecs in /usr/local/lib/python3.8/dist-packages (2023.1.23)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.6.0) (1.6.3)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.6.0) (1.1.2)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.6.0) (0.15.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.6.0) (1.12)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.37.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.6.0) (1.51.3)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.6.0) (0.38.4)\n",
            "Requirement already satisfied: tensorflow-estimator~=2.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.6.0) (2.11.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.6.0) (0.2.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.6.0) (1.15.0)\n",
            "Collecting numpy\n",
            "  Using cached numpy-1.19.5-cp38-cp38-manylinux2010_x86_64.whl (14.9 MB)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.6.0) (3.19.6)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.6.0) (3.7.4.3)\n",
            "Requirement already satisfied: keras~=2.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.6.0) (2.11.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.6.0) (1.1.0)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.6.0) (0.4.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.6.0) (1.12.1)\n",
            "Requirement already satisfied: clang~=5.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.6.0) (5.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.6.0) (2.11.2)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.6.0) (3.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (4.38.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (8.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow==2.6.0) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow==2.6.0) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow==2.6.0) (3.4.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow==2.6.0) (2.2.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow==2.6.0) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow==2.6.0) (2.25.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow==2.6.0) (0.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow==2.6.0) (2.16.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.6.0) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.6.0) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.6.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.6.0) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.6.0) (6.0.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.6.0) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.6.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.6.0) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.6.0) (1.26.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.8/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.6->tensorflow==2.6.0) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.6.0) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.6.0) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.6.0) (3.2.2)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.20.0\n",
            "    Uninstalling numpy-1.20.0:\n",
            "      Successfully uninstalled numpy-1.20.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xarray 2022.12.0 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\n",
            "xarray-einstats 0.5.1 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\n",
            "jaxlib 0.4.4+cuda11.cudnn82 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\n",
            "jax 0.4.4 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\n",
            "cmdstanpy 1.1.0 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\n",
            "bokeh 2.4.3 requires typing-extensions>=3.10.0, but you have typing-extensions 3.7.4.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.19.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l test/mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlWhNadA1-U9",
        "outputId": "88ebf9df-f95b-407e-893f-dc0b6e0e82b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 70\n",
            "-rw------- 1 root root 3301 Jan 20  2004 01_test_mask.gif\n",
            "-rw------- 1 root root 3304 Jan 20  2004 02_test_mask.gif\n",
            "-rw------- 1 root root 3309 Jan 20  2004 03_test_mask.gif\n",
            "-rw------- 1 root root 3312 Jan 20  2004 04_test_mask.gif\n",
            "-rw------- 1 root root 3304 Jan 20  2004 05_test_mask.gif\n",
            "-rw------- 1 root root 3301 Jan 20  2004 06_test_mask.gif\n",
            "-rw------- 1 root root 3310 Jan 20  2004 07_test_mask.gif\n",
            "-rw------- 1 root root 3303 Jan 20  2004 08_test_mask.gif\n",
            "-rw------- 1 root root 3309 Jan 20  2004 09_test_mask.gif\n",
            "-rw------- 1 root root 3304 Jan 20  2004 10_test_mask.gif\n",
            "-rw------- 1 root root 3307 Jan 20  2004 11_test_mask.gif\n",
            "-rw------- 1 root root 3304 Jan 20  2004 12_test_mask.gif\n",
            "-rw------- 1 root root 3300 Jan 20  2004 13_test_mask.gif\n",
            "-rw------- 1 root root 3306 Jan 20  2004 14_test_mask.gif\n",
            "-rw------- 1 root root 3306 Jan 20  2004 15_test_mask.gif\n",
            "-rw------- 1 root root 3310 Jan 20  2004 16_test_mask.gif\n",
            "-rw------- 1 root root 3304 Jan 20  2004 17_test_mask.gif\n",
            "-rw------- 1 root root 3306 Jan 20  2004 18_test_mask.gif\n",
            "-rw------- 1 root root 3303 Jan 20  2004 19_test_mask.gif\n",
            "-rw------- 1 root root 3307 Jan 20  2004 20_test_mask.gif\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tifffile"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYEYh-mP54ol",
        "outputId": "0681b7fa-4582-40c5-82f0-0412ee575eeb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tifffile in /usr/local/lib/python3.8/dist-packages (2023.2.27)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from tifffile) (1.19.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tifffile\n",
        "import imageio\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define the paths to the training and testing images and masks\n",
        "train_images_path = './training/images/'\n",
        "train_masks_path = './training/mask/'\n",
        "test_images_path = './test/images/'\n",
        "test_masks_path = './test/mask/'\n",
        "\n",
        "# Define the image size\n",
        "IMG_HEIGHT = 256\n",
        "IMG_WIDTH = 256\n",
        "\n",
        "# Define the number of channels in the images and masks\n",
        "IMG_CHANNELS = 3\n",
        "MASK_CHANNELS = 1\n",
        "\n",
        "# Load the training images and masks\n",
        "train_images = sorted(os.listdir(train_images_path))\n",
        "train_masks = sorted(os.listdir(train_masks_path))\n",
        "\n",
        "# Create empty arrays for the training images and masks\n",
        "X_train = np.zeros((len(train_images), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\n",
        "Y_train = np.zeros((len(train_masks), IMG_HEIGHT, IMG_WIDTH, MASK_CHANNELS), dtype=np.uint8)\n",
        "\n",
        "# Load the training images and masks into the arrays\n",
        "for i, image_path in enumerate(train_images):\n",
        "    img = tifffile.imread(os.path.join(train_images_path, image_path))\n",
        "    img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))\n",
        "    X_train[i] = img\n",
        "    \n",
        "for i, mask_path in enumerate(train_masks):\n",
        "    mask = imageio.imread(os.path.join(train_masks_path, mask_path))\n",
        "    mask = cv2.resize(mask, (IMG_WIDTH, IMG_HEIGHT))\n",
        "    mask = np.expand_dims(mask, axis=-1)\n",
        "    Y_train[i] = mask\n",
        "\n",
        "# Load the testing images and masks\n",
        "test_images = sorted(os.listdir(test_images_path))\n",
        "test_masks = sorted(os.listdir(test_masks_path))\n",
        "\n",
        "\n",
        "# Create empty arrays for the testing images and masks\n",
        "X_test = np.zeros((len(test_images), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\n",
        "Y_test = np.zeros((len(test_masks), IMG_HEIGHT, IMG_WIDTH, MASK_CHANNELS), dtype=np.uint8)\n",
        "\n",
        "# Load the testing images and masks into the arrays\n",
        "for i, image_path in enumerate(test_images):\n",
        "    img = tifffile.imread(os.path.join(test_images_path, image_path))\n",
        "    img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))\n",
        "    X_test[i] = img\n",
        "    \n",
        "for i, mask_path in enumerate(test_masks):\n",
        "    mask = imageio.imread(os.path.join(test_masks_path, mask_path))\n",
        "    mask = cv2.resize(mask, (IMG_WIDTH, IMG_HEIGHT))\n",
        "    mask = np.expand_dims(mask, axis=-1)\n",
        "    Y_test[i] = mask\n"
      ],
      "metadata": {
        "id": "-6u63pp4yPGu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall tensorflow\n",
        "!pip install --upgrade tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pKmLpY5MEUjx",
        "outputId": "16a3eb44-3a90-4f72-8c05-3677c7f2c496"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.11.0\n",
            "Uninstalling tensorflow-2.11.0:\n",
            "  Would remove:\n",
            "    /usr/local/bin/estimator_ckpt_converter\n",
            "    /usr/local/bin/import_pb_to_tensorboard\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.8/dist-packages/tensorflow-2.11.0.dist-info/*\n",
            "    /usr/local/lib/python3.8/dist-packages/tensorflow/*\n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled tensorflow-2.11.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow\n",
            "  Using cached tensorflow-2.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.19.6)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.11.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.24.2)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.11.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.51.3)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (15.0.6.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow) (23.0)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.11.2)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (23.3.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.31.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.7.4.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.16.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.25.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (5.3.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (6.0.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2.10)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.8/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (3.2.2)\n",
            "Installing collected packages: tensorflow\n",
            "Successfully installed tensorflow-2.11.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dropout, UpSampling2D, concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Define the input shape of the model\n",
        "input_shape = (IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)\n",
        "\n",
        "# Define the number of filters in the first convolutional layer\n",
        "n_filters = 32\n",
        "\n",
        "# Define the U-Net model\n",
        "inputs = Input(input_shape)\n",
        "\n",
        "# Contracting path\n",
        "c1 = Conv2D(n_filters, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(inputs)\n",
        "c1 = Conv2D(n_filters, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)\n",
        "p1 = MaxPooling2D((2, 2))(c1)\n",
        "p1 = Dropout(0.1)(p1)\n",
        "\n",
        "c2 = Conv2D(n_filters*2, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)\n",
        "c2 = Conv2D(n_filters*2, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)\n",
        "p2 = MaxPooling2D((2, 2))(c2)\n",
        "p2 = Dropout(0.1)(p2)\n",
        "\n",
        "c3 = Conv2D(n_filters*4, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p2)\n",
        "c3 = Conv2D(n_filters*4, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)\n",
        "p3 = MaxPooling2D((2, 2))(c3)\n",
        "p3 = Dropout(0.2)(p3)\n",
        "\n",
        "c4 = Conv2D(n_filters*8, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p3)\n",
        "c4 = Conv2D(n_filters*8, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c4)\n",
        "p4 = MaxPooling2D((2, 2))(c4)\n",
        "p4 = Dropout(0.2)(p4)\n",
        "\n",
        "c5 = Conv2D(n_filters*16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p4)\n",
        "c5 = Conv2D(n_filters*16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)\n",
        "p5 = MaxPooling2D((2, 2))(c5)\n",
        "p5 = Dropout(0.3)(p5)\n",
        "\n",
        "# Expansive path\n",
        "u6 = UpSampling2D((2, 2))(p5)\n",
        "c6 = Conv2D(n_filters*8, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u6)\n",
        "c6 = concatenate([c6, c5])\n",
        "c6 = Conv2D(n_filters*8, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c6)\n",
        "c6 = Conv2D(n_filters*8, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c6)\n",
        "c6 = Dropout(0.2)(c6)\n",
        "\n",
        "u7 = UpSampling2D((2, 2))(c6)\n",
        "c7 = Conv2D(n_filters*4, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)\n",
        "c7 = concatenate([c7, c4])\n",
        "c7 = Conv2D(n_filters*4, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)\n",
        "c7 = Conv2D(n_filters*4, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)\n",
        "c7 = Dropout(0.2)(c7)\n",
        "\n",
        "u8 = UpSampling2D((2, 2))(c7)\n",
        "c8 = Conv2D(n_filters*2, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u8)\n",
        "c8 = concatenate([c8, c3])\n",
        "c8 = Conv2D(n_filters*2, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8)\n",
        "c8 = Conv2D(n_filters*2, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8)\n",
        "c8 = Dropout(0.1)(c8)\n",
        "\n",
        "u9 = UpSampling2D((2, 2))(c8)\n",
        "c9 = Conv2D(n_filters, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u9)\n",
        "c9 = concatenate([c9, c2])\n",
        "c9 = Conv2D(n_filters, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9)\n",
        "c9 = Conv2D(n_filters, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9)\n",
        "c9 = Dropout(0.1)(c9)\n",
        "\n",
        "u10 = UpSampling2D((2, 2))(c9)\n",
        "c10 = Conv2D(n_filters, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u10)\n",
        "c10 = concatenate([c10, c1])\n",
        "c10 = Conv2D(n_filters, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c10)\n",
        "c10 = Conv2D(n_filters, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c10)\n",
        "\n",
        "outputs = Conv2D(1, (1, 1), activation='sigmoid')(c10)\n",
        "\n",
        "model = Model(inputs=[inputs], outputs=[outputs])\n"
      ],
      "metadata": {
        "id": "vSBVbjW48f-8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhU-l6zboi7y",
        "outputId": "46d16ba8-f214-4347-be32-d52256134492"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: Transport endpoint is not connected\n",
            "pwd: error retrieving current directory: getcwd: cannot access parent directories: Transport endpoint is not connected\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "\n",
        "# Define the batch size and number of epochs\n",
        "BATCH_SIZE = 4\n",
        "EPOCHS = 50\n",
        "\n",
        "# Define the training and validation data generators\n",
        "train_data_gen = ImageDataGenerator(rescale=1./255)\n",
        "val_data_gen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_data_gen.flow_from_directory(\n",
        "    './training/',\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode=None,\n",
        "    seed=42,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_generator = val_data_gen.flow_from_directory(\n",
        "    './test/',\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode=None,\n",
        "    seed=42,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Define the model checkpoint\n",
        "checkpoint_path = \"./checkpoint/model.ckpt\"\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    filepath=checkpoint_path,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    save_best_only=True\n",
        ")\n",
        "\n",
        "\n",
        "# Compile the model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Set the learning rate and optimizer\n",
        "lr = 1e-4\n",
        "optimizer = Adam(lr=lr)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=len(train_generator),\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=val_generator,\n",
        "    validation_steps=len(val_generator),\n",
        "    callbacks=[checkpoint_callback]\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "id": "s0KEQDY9E0-L",
        "outputId": "b7c18872-2a02-4439-bd7d-15914e8a5e00"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20 images belonging to 3 classes.\n",
            "Found 20 images belonging to 2 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-8e96e90c4c95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1025, in train_step\n        self._validate_target_and_loss(y, loss)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 979, in _validate_target_and_loss\n        raise ValueError(\n\n    ValueError: Target data is missing. Your model was compiled with loss=binary_crossentropy, and therefore expects target data to be provided in `fit()`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "# Define the batch size, number of epochs, and image dimensions\n",
        "BATCH_SIZE = 4\n",
        "EPOCHS = 50\n",
        "IMG_HEIGHT = 224\n",
        "IMG_WIDTH = 224\n",
        "\n",
        "# Define the training and validation data generators\n",
        "train_data_gen = ImageDataGenerator(rescale=1./255)\n",
        "val_data_gen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_data_gen.flow_from_directory(\n",
        "    './training/',\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary',\n",
        "    seed=42,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_generator = val_data_gen.flow_from_directory(\n",
        "    './test/',\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary',\n",
        "    seed=42,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Define the model checkpoint\n",
        "checkpoint_path = \"./checkpoint/model.ckpt\"\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    filepath=checkpoint_path,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    save_best_only=True\n",
        ")\n",
        "\n",
        "# Define the model\n",
        "model = Sequential([\n",
        "    Conv2D(16, 3, padding='same', activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH ,3)),\n",
        "    MaxPooling2D(),\n",
        "    Conv2D(32, 3, padding='same', activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Conv2D(64, 3, padding='same', activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Set the learning rate and optimizer\n",
        "lr = 1e-4\n",
        "optimizer = Adam(learning_rate=lr)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=len(train_generator),\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=val_generator,\n",
        "    validation_steps=len(val_generator),\n",
        "    callbacks=[checkpoint_callback]\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mt0cmpsmSSLt",
        "outputId": "2f6512d8-7e01-4553-890e-3aa0c0b5309c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20 images belonging to 3 classes.\n",
            "Found 20 images belonging to 2 classes.\n",
            "Epoch 1/50\n",
            "5/5 [==============================] - 10s 2s/step - loss: 0.2135 - accuracy: 1.0000 - val_loss: 5.2047 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/50\n",
            "5/5 [==============================] - 7s 2s/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 9.0003 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 1.9878e-04 - accuracy: 1.0000 - val_loss: 11.6496 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/50\n",
            "5/5 [==============================] - 6s 1s/step - loss: 3.1638e-05 - accuracy: 1.0000 - val_loss: 13.4263 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/50\n",
            "5/5 [==============================] - 6s 1s/step - loss: 1.5818e-05 - accuracy: 1.0000 - val_loss: 14.6001 - val_accuracy: 0.0000e+00\n",
            "Epoch 6/50\n",
            "5/5 [==============================] - 7s 1s/step - loss: 6.6110e-06 - accuracy: 1.0000 - val_loss: 15.3628 - val_accuracy: 0.0000e+00\n",
            "Epoch 7/50\n",
            "5/5 [==============================] - 5s 997ms/step - loss: 4.2669e-06 - accuracy: 1.0000 - val_loss: 15.8533 - val_accuracy: 0.0000e+00\n",
            "Epoch 8/50\n",
            "5/5 [==============================] - 7s 1s/step - loss: 3.0578e-06 - accuracy: 1.0000 - val_loss: 16.1656 - val_accuracy: 0.0000e+00\n",
            "Epoch 9/50\n",
            "5/5 [==============================] - 5s 1s/step - loss: 2.6114e-06 - accuracy: 1.0000 - val_loss: 16.3637 - val_accuracy: 0.0000e+00\n",
            "Epoch 10/50\n",
            "5/5 [==============================] - 6s 1s/step - loss: 2.3326e-06 - accuracy: 1.0000 - val_loss: 16.4887 - val_accuracy: 0.0000e+00\n",
            "Epoch 11/50\n",
            "5/5 [==============================] - 6s 1s/step - loss: 2.1767e-06 - accuracy: 1.0000 - val_loss: 16.5678 - val_accuracy: 0.0000e+00\n",
            "Epoch 12/50\n",
            "5/5 [==============================] - 7s 1s/step - loss: 2.0317e-06 - accuracy: 1.0000 - val_loss: 16.6175 - val_accuracy: 0.0000e+00\n",
            "Epoch 13/50\n",
            "5/5 [==============================] - 6s 1s/step - loss: 1.9755e-06 - accuracy: 1.0000 - val_loss: 16.6492 - val_accuracy: 0.0000e+00\n",
            "Epoch 14/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 1.9500e-06 - accuracy: 1.0000 - val_loss: 16.6703 - val_accuracy: 0.0000e+00\n",
            "Epoch 15/50\n",
            "5/5 [==============================] - 6s 1s/step - loss: 1.9226e-06 - accuracy: 1.0000 - val_loss: 16.6843 - val_accuracy: 0.0000e+00\n",
            "Epoch 16/50\n",
            "5/5 [==============================] - 7s 1s/step - loss: 1.9091e-06 - accuracy: 1.0000 - val_loss: 16.6945 - val_accuracy: 0.0000e+00\n",
            "Epoch 17/50\n",
            "5/5 [==============================] - 5s 1s/step - loss: 1.8941e-06 - accuracy: 1.0000 - val_loss: 16.7017 - val_accuracy: 0.0000e+00\n",
            "Epoch 18/50\n",
            "5/5 [==============================] - 7s 2s/step - loss: 1.8872e-06 - accuracy: 1.0000 - val_loss: 16.7078 - val_accuracy: 0.0000e+00\n",
            "Epoch 19/50\n",
            "5/5 [==============================] - 6s 1s/step - loss: 1.8772e-06 - accuracy: 1.0000 - val_loss: 16.7124 - val_accuracy: 0.0000e+00\n",
            "Epoch 20/50\n",
            "5/5 [==============================] - 5s 985ms/step - loss: 1.8708e-06 - accuracy: 1.0000 - val_loss: 16.7163 - val_accuracy: 0.0000e+00\n",
            "Epoch 21/50\n",
            "5/5 [==============================] - 8s 2s/step - loss: 1.8667e-06 - accuracy: 1.0000 - val_loss: 16.7206 - val_accuracy: 0.0000e+00\n",
            "Epoch 22/50\n",
            "5/5 [==============================] - 5s 1s/step - loss: 1.8613e-06 - accuracy: 1.0000 - val_loss: 16.7247 - val_accuracy: 0.0000e+00\n",
            "Epoch 23/50\n",
            "5/5 [==============================] - 5s 1s/step - loss: 1.8555e-06 - accuracy: 1.0000 - val_loss: 16.7286 - val_accuracy: 0.0000e+00\n",
            "Epoch 24/50\n",
            "5/5 [==============================] - 5s 1s/step - loss: 1.8506e-06 - accuracy: 1.0000 - val_loss: 16.7325 - val_accuracy: 0.0000e+00\n",
            "Epoch 25/50\n",
            "5/5 [==============================] - 6s 1s/step - loss: 1.8455e-06 - accuracy: 1.0000 - val_loss: 16.7366 - val_accuracy: 0.0000e+00\n",
            "Epoch 26/50\n",
            "5/5 [==============================] - 5s 1s/step - loss: 1.8408e-06 - accuracy: 1.0000 - val_loss: 16.7408 - val_accuracy: 0.0000e+00\n",
            "Epoch 27/50\n",
            "5/5 [==============================] - 6s 1s/step - loss: 1.8351e-06 - accuracy: 1.0000 - val_loss: 16.7451 - val_accuracy: 0.0000e+00\n",
            "Epoch 28/50\n",
            "5/5 [==============================] - 6s 1s/step - loss: 1.8292e-06 - accuracy: 1.0000 - val_loss: 16.7491 - val_accuracy: 0.0000e+00\n",
            "Epoch 29/50\n",
            "5/5 [==============================] - 14s 3s/step - loss: 1.8242e-06 - accuracy: 1.0000 - val_loss: 16.7534 - val_accuracy: 0.0000e+00\n",
            "Epoch 30/50\n",
            "5/5 [==============================] - 6s 1s/step - loss: 1.8178e-06 - accuracy: 1.0000 - val_loss: 16.7574 - val_accuracy: 0.0000e+00\n",
            "Epoch 31/50\n",
            "5/5 [==============================] - 6s 1s/step - loss: 1.8137e-06 - accuracy: 1.0000 - val_loss: 16.7620 - val_accuracy: 0.0000e+00\n",
            "Epoch 32/50\n",
            "5/5 [==============================] - 6s 1s/step - loss: 1.8059e-06 - accuracy: 1.0000 - val_loss: 16.7657 - val_accuracy: 0.0000e+00\n",
            "Epoch 33/50\n",
            "5/5 [==============================] - 6s 1s/step - loss: 1.8019e-06 - accuracy: 1.0000 - val_loss: 16.7703 - val_accuracy: 0.0000e+00\n",
            "Epoch 34/50\n",
            "5/5 [==============================] - 7s 1s/step - loss: 1.7964e-06 - accuracy: 1.0000 - val_loss: 16.7751 - val_accuracy: 0.0000e+00\n",
            "Epoch 35/50\n",
            "5/5 [==============================] - 7s 1s/step - loss: 1.7911e-06 - accuracy: 1.0000 - val_loss: 16.7801 - val_accuracy: 0.0000e+00\n",
            "Epoch 36/50\n",
            "5/5 [==============================] - 7s 1s/step - loss: 1.7841e-06 - accuracy: 1.0000 - val_loss: 16.7848 - val_accuracy: 0.0000e+00\n",
            "Epoch 37/50\n",
            "5/5 [==============================] - 7s 1s/step - loss: 1.7791e-06 - accuracy: 1.0000 - val_loss: 16.7898 - val_accuracy: 0.0000e+00\n",
            "Epoch 38/50\n",
            "5/5 [==============================] - 5s 1s/step - loss: 1.7706e-06 - accuracy: 1.0000 - val_loss: 16.7940 - val_accuracy: 0.0000e+00\n",
            "Epoch 39/50\n",
            "5/5 [==============================] - 6s 1s/step - loss: 1.7659e-06 - accuracy: 1.0000 - val_loss: 16.7988 - val_accuracy: 0.0000e+00\n",
            "Epoch 40/50\n",
            "5/5 [==============================] - 8s 2s/step - loss: 1.7610e-06 - accuracy: 1.0000 - val_loss: 16.8042 - val_accuracy: 0.0000e+00\n",
            "Epoch 41/50\n",
            "5/5 [==============================] - 5s 1s/step - loss: 1.7529e-06 - accuracy: 1.0000 - val_loss: 16.8090 - val_accuracy: 0.0000e+00\n",
            "Epoch 42/50\n",
            "5/5 [==============================] - 6s 1s/step - loss: 1.7478e-06 - accuracy: 1.0000 - val_loss: 16.8142 - val_accuracy: 0.0000e+00\n",
            "Epoch 43/50\n",
            "5/5 [==============================] - 8s 2s/step - loss: 1.7417e-06 - accuracy: 1.0000 - val_loss: 16.8196 - val_accuracy: 0.0000e+00\n",
            "Epoch 44/50\n",
            "5/5 [==============================] - 5s 1s/step - loss: 1.7345e-06 - accuracy: 1.0000 - val_loss: 16.8248 - val_accuracy: 0.0000e+00\n",
            "Epoch 45/50\n",
            "5/5 [==============================] - 7s 2s/step - loss: 1.7273e-06 - accuracy: 1.0000 - val_loss: 16.8297 - val_accuracy: 0.0000e+00\n",
            "Epoch 46/50\n",
            "5/5 [==============================] - 6s 1s/step - loss: 1.7207e-06 - accuracy: 1.0000 - val_loss: 16.8345 - val_accuracy: 0.0000e+00\n",
            "Epoch 47/50\n",
            "5/5 [==============================] - 6s 1s/step - loss: 1.7153e-06 - accuracy: 1.0000 - val_loss: 16.8399 - val_accuracy: 0.0000e+00\n",
            "Epoch 48/50\n",
            "5/5 [==============================] - 6s 1s/step - loss: 1.7093e-06 - accuracy: 1.0000 - val_loss: 16.8457 - val_accuracy: 0.0000e+00\n",
            "Epoch 49/50\n",
            "5/5 [==============================] - 5s 1s/step - loss: 1.7015e-06 - accuracy: 1.0000 - val_loss: 16.8510 - val_accuracy: 0.0000e+00\n",
            "Epoch 50/50\n",
            "5/5 [==============================] - 6s 1s/step - loss: 1.6978e-06 - accuracy: 1.0000 - val_loss: 16.8574 - val_accuracy: 0.0000e+00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save the trained model\n",
        "model.save('unet_model.h5')\n"
      ],
      "metadata": {
        "id": "TQthTFl3ER5s"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the saved model\n",
        "model = load_model('unet_model.h5')\n",
        "\n",
        "# Use the model to make predictions on test data\n",
        "test_data_gen = ImageDataGenerator(rescale=1./255)\n",
        "test_generator = test_data_gen.flow_from_directory(\n",
        "    './test/',\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode=None,\n",
        "    seed=42,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "predictions = model.predict(test_generator)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEfaSprYT6Kn",
        "outputId": "f0887368-727f-490b-831b-e7e4d94b2ffd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20 images belonging to 2 classes.\n",
            "5/5 [==============================] - 1s 133ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-xGx2u2UMEz",
        "outputId": "54e48198-1dcf-4029-9b15-8edd99cc2fad"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.        ]\n",
            " [1.        ]\n",
            " [0.9999897 ]\n",
            " [1.        ]\n",
            " [0.99999934]\n",
            " [0.99999994]\n",
            " [1.        ]\n",
            " [0.9999999 ]\n",
            " [1.        ]\n",
            " [0.99999917]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [0.99999994]\n",
            " [0.9999943 ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [1.        ]\n",
            " [0.999999  ]\n",
            " [0.99999994]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# Set the threshold for binarization\n",
        "threshold = 0.5\n",
        "\n",
        "# Generate predictions for the test data\n",
        "y_pred = model.predict(test_generator)\n",
        "\n",
        "# Iterate over the predictions and save binary PNG files\n",
        "for i, pred in enumerate(y_pred):\n",
        "    # Binarize the prediction using the threshold\n",
        "    pred[pred >= threshold] = 1\n",
        "    pred[pred < threshold] = 0\n",
        "    # Convert the prediction to a PIL image and save it as PNG\n",
        "    pred_img = Image.fromarray(np.uint8(pred*255), mode='L')\n",
        "    pred_img.save(f'model/{i+1}.png')\n",
        "\n",
        "    # Calculate the dice coefficient for the corresponding ground truth image\n",
        "    mask_path = f'model/{i+1}.png'\n",
        "    mask = np.array(Image.open(mask_path).convert('L'))\n",
        "    mask[mask >= 1] = 1\n",
        "    intersection = np.sum(np.logical_and(mask, pred))\n",
        "    union = np.sum(np.logical_or(mask, pred))\n",
        "    dice = 2 * intersection / (union + intersection)\n",
        "    print(f'Dice coefficient for image {i+1}: {dice:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAO9agdrU4BK",
        "outputId": "ee755e99-9492-4c60-9a8e-ff6cd17fbd24"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 1s 198ms/step\n",
            "Dice coefficient for image 1: 1.0000\n",
            "Dice coefficient for image 2: 1.0000\n",
            "Dice coefficient for image 3: 1.0000\n",
            "Dice coefficient for image 4: 1.0000\n",
            "Dice coefficient for image 5: 1.0000\n",
            "Dice coefficient for image 6: 1.0000\n",
            "Dice coefficient for image 7: 1.0000\n",
            "Dice coefficient for image 8: 1.0000\n",
            "Dice coefficient for image 9: 1.0000\n",
            "Dice coefficient for image 10: 1.0000\n",
            "Dice coefficient for image 11: 1.0000\n",
            "Dice coefficient for image 12: 1.0000\n",
            "Dice coefficient for image 13: 1.0000\n",
            "Dice coefficient for image 14: 1.0000\n",
            "Dice coefficient for image 15: 1.0000\n",
            "Dice coefficient for image 16: 1.0000\n",
            "Dice coefficient for image 17: 1.0000\n",
            "Dice coefficient for image 18: 1.0000\n",
            "Dice coefficient for image 19: 1.0000\n",
            "Dice coefficient for image 20: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yZYRYim41cRh"
      }
    }
  ]
}